---
alwaysApply: true
---
---
description: Context and constraints for MATS Mechanistic Interpretability project
globs: *.py, *.ipynb
---

# MATS Mechanistic Interpretability Project

**Goal:** Produce original, compelling mech-interp research. Current directions include sycophancy steering, unfaithful CoT classification, and synthetic knowledge insertion—but follow interesting findings wherever they lead.

## Hardware

- **GPU:** NVIDIA RTX 6000 Ada (48GB VRAM)
- **Implication:** No need for quantization. Load models in `bfloat16` or `float16`. Quantization hinders interpretability analysis.

## Environment Setup

- **Cache Location:** HuggingFace cache is redirected to `/workspace/.cache/huggingface` (root filesystem has limited space)
- **Always import config first:** Start experiment files with:
  ```python
  from experiments.config import *  # Sets HF_HOME before any HF imports
  ```
- **Or if running standalone scripts**, ensure this is at the top before any transformers/huggingface imports:
  ```python
  import os
  os.environ["HF_HOME"] = "/workspace/.cache/huggingface"
  ```

## Tech Stack

This is what I use so far. However, I am also not an expert, so especially when something feels inefficient, I encourage you to use search to check, and suggest improvements!

- **TransformerLens:** For interpretability on supported architectures (rich hook API)
- **nnsight:** For activation access on HuggingFace models when TransformerLens doesn't support the architecture
- **Framework:** PyTorch
- **Other libraries as needed:** `peft`/`trl` for fine-tuning comparisons, standard HuggingFace when needed

## Models Under Investigation

- **Primary:** `meta-llama/Meta-Llama-3-8B-Instruct` (or smaller models like Qwen-2.5-1.5B for faster iteration)
- **Distillation comparison:**
  - Base: `Qwen/Qwen2.5-Math-7B`
  - Distilled: `deepseek-ai/DeepSeek-R1-Distill-Qwen-7B`
  - Note: Both share identical vocab (151,643 tokens), but DeepSeek adds a BOS token by default. Use `add_special_tokens=False` for token-aligned comparisons.

## TransformerLens Conventions

When using TransformerLens:

- Use `model.run_with_hooks()` or `model.run_with_cache()` for interventions
- Call `model.reset_hooks()` before experiments to clear stale hooks
- Use standard naming: `resid_pre`, `resid_post`, `W_U`, `W_E`

## Priorities

1. **Speed of iteration** — Get results fast, clean up later if needed
2. **Sound experiments** — Controls, baselines, sanity checks matter more than clean code
3. **Flexibility** — Don't over-engineer; the direction may change based on findings

## Folder Conventions

- Experiments are in `experiments/exp_NNN_description/`
- Each experiment folder contains related scripts and notebooks
- Import shared config via `from experiments.config import *` 