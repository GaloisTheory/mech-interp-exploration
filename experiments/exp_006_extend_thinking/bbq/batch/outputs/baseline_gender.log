============================================================
BBQ BATCH EXPERIMENT
============================================================

Experiment: baseline_gender
Config: /workspace/experiments/exp_006_extend_thinking/bbq/batch/configs/baseline_gender.yaml

------------------------------------------------------------
LOADING QUESTIONS
------------------------------------------------------------
Loaded 10 BBQ questions from 1 categories

------------------------------------------------------------
LOADING MODEL
------------------------------------------------------------
Loading: Qwen/Qwen3-8B
This may take a minute...
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:37<02:31, 37.81s/it]Loading checkpoint shards:  40%|████      | 2/5 [01:14<01:51, 37.25s/it]Loading checkpoint shards:  60%|██████    | 3/5 [01:49<01:12, 36.17s/it]Loading checkpoint shards:  80%|████████  | 4/5 [02:18<00:33, 33.22s/it]Loading checkpoint shards: 100%|██████████| 5/5 [02:29<00:00, 25.21s/it]Loading checkpoint shards: 100%|██████████| 5/5 [02:29<00:00, 29.85s/it]
Model loaded on cuda:0

------------------------------------------------------------
GENERATION CONFIG
------------------------------------------------------------
Temperature: 0.6, Top-p: 0.95
Max tokens: 2000
Enable thinking: True
Few-shot: True
Token to match: </think>
Intercept count: 0
Samples per question: 10

============================================================
RUNNING EXPERIMENT
============================================================
Questions:   0%|          | 0/10 [00:00<?, ?it/s][Config] Token to match: </think> (ID: 151668)
[Config] Override text: <dynamic per intercept>
[Config] Intercept count: 0
[Config] Enable thinking: True

Questions:   0%|          | 0/10 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/workspace/experiments/exp_006_extend_thinking/bbq/batch/run_question_batch.py", line 302, in <module>
    run_experiment(args.config, resume=args.resume, dry_run=args.dry_run)
  File "/workspace/experiments/exp_006_extend_thinking/bbq/batch/run_question_batch.py", line 221, in run_experiment
    result = generate_with_custom_override(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/experiments/exp_006_extend_thinking/shared/generation.py", line 832, in generate_with_custom_override
    outputs = hf_model(
              ^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py", line 918, in wrapper
    output = func(self, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py", line 494, in forward
    logits = self.lm_head(hidden_states[:, slice_indices, :])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 146.00 MiB. GPU 0 has a total capacity of 178.36 GiB of which 79.50 MiB is free. Process 31609 has 16.32 GiB memory in use. Including non-PyTorch memory, this process has 16.12 GiB memory in use. Process 31605 has 16.17 GiB memory in use. Process 31607 has 16.33 GiB memory in use. Process 31601 has 16.07 GiB memory in use. Process 31604 has 16.32 GiB memory in use. Process 31606 has 16.09 GiB memory in use. Process 31608 has 16.32 GiB memory in use. Process 31602 has 16.16 GiB memory in use. Process 32962 has 16.15 GiB memory in use. Process 32961 has 16.17 GiB memory in use. Of the allocated memory 15.34 GiB is allocated by PyTorch, and 24.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
